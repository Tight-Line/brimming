<%= form_with model: [:admin, @llm_provider], class: "form", data: { controller: "ollama-models", ollama_models_provider_type_value: @llm_provider.provider_type } do |f| %>
  <% if @llm_provider.errors.any? %>
    <div class="alert alert-danger">
      <h4><%= pluralize(@llm_provider.errors.count, "error") %> prevented this provider from being saved:</h4>
      <ul>
        <% @llm_provider.errors.full_messages.each do |message| %>
          <li><%= message %></li>
        <% end %>
      </ul>
    </div>
  <% end %>

  <% unless @llm_provider.persisted? %>
    <%= f.hidden_field :provider_type %>
  <% end %>

  <fieldset>
    <legend>Provider Settings</legend>

    <div class="form-group">
      <label>Provider Type</label>
      <div class="form-control-static"><%= @llm_provider.display_provider_type %></div>
      <% if @llm_provider.persisted? %>
        <small class="form-text text-muted">
          Provider type cannot be changed after creation.
        </small>
      <% end %>
    </div>

    <div class="form-group">
      <%= f.label :name, "Display Name" %>
      <%= f.text_field :name, class: "form-control", placeholder: "e.g., Production GPT-4" %>
      <small class="form-text text-muted">A friendly name to identify this configuration</small>
    </div>

    <div class="form-group" data-ollama-models-target="modelGroup">
      <%= f.label :llm_model, "Model" %>
      <% if @llm_provider.persisted? %>
        <div class="form-control-static"><%= @llm_provider.llm_model %></div>
        <%= f.hidden_field :llm_model %>
        <small class="form-text text-muted">
          Model cannot be changed after creation. To use a different model,
          <%= link_to "create a new provider", new_admin_llm_provider_path %> and set it as default.
        </small>
      <% elsif @llm_provider.uses_dynamic_models? %>
        <%# Ollama: dynamic model discovery with text input fallback %>
        <div class="ollama-model-selector">
          <select id="llm_provider_llm_model_select"
                  class="form-control"
                  data-ollama-models-target="modelSelect"
                  data-action="change->ollama-models#selectModel">
            <option value="">Loading models...</option>
          </select>
          <%= f.text_field :llm_model,
                           class: "form-control mt-2",
                           placeholder: "Or enter model name (e.g., llama3.2:latest)",
                           data: { ollama_models_target: "modelInput" } %>
          <small class="form-text text-muted" data-ollama-models-target="modelStatus">
            Detecting Ollama instance...
          </small>
        </div>
      <% else %>
        <%# Other providers: static model list %>
        <% models = @llm_provider.available_models %>
        <% if models.any? %>
          <%= f.select :llm_model,
                       options_for_select(models, @llm_provider.llm_model),
                       {},
                       class: "form-control" %>
        <% else %>
          <%= f.text_field :llm_model, class: "form-control", placeholder: "model-name" %>
          <small class="form-text text-muted">Enter the model identifier</small>
        <% end %>
      <% end %>
    </div>
  </fieldset>

  <fieldset>
    <legend>API Credentials</legend>

    <% if @llm_provider.requires_api_key? %>
      <div class="form-group">
        <%= f.label :api_key, "API Key" %>
        <%= f.password_field :api_key, class: "form-control", autocomplete: "new-password" %>
        <% if @llm_provider.persisted? && @llm_provider.api_key.present? %>
          <small class="form-text text-muted">Leave blank to keep current API key</small>
        <% end %>
      </div>
    <% end %>

    <% if @llm_provider.requires_api_endpoint? %>
      <div class="form-group">
        <%= f.label :api_endpoint, "API Endpoint" %>
        <%= f.text_field :api_endpoint,
                         class: "form-control",
                         data: @llm_provider.uses_dynamic_models? ? { ollama_models_target: "endpointInput", action: "change->ollama-models#endpointChanged" } : {} %>
        <small class="form-text text-muted">
          <% if @llm_provider.provider_type == "ollama" %>
            The URL of your Ollama server (use host.docker.internal instead of localhost if running in Docker)
          <% else %>
            Your Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com)
          <% end %>
        </small>
      </div>
    <% end %>

    <% unless @llm_provider.requires_api_key? || @llm_provider.requires_api_endpoint? %>
      <p class="text-muted">This provider type does not require API credentials.</p>
    <% end %>
  </fieldset>

  <fieldset>
    <legend>Generation Settings</legend>

    <div class="form-group">
      <%= f.label :temperature, "Temperature" %>
      <%= f.number_field :temperature,
                         value: @llm_provider.temperature,
                         class: "form-control",
                         step: 0.1,
                         min: 0,
                         max: 2 %>
      <small class="form-text text-muted">
        Controls randomness in generation (0-2). Lower values are more focused and deterministic.
        Default: 0.7
      </small>
    </div>

    <div class="form-group">
      <%= f.label :max_tokens, "Max Tokens" %>
      <%= f.number_field :max_tokens,
                         value: @llm_provider.max_tokens,
                         class: "form-control",
                         step: 256,
                         min: 256,
                         max: 128000 %>
      <small class="form-text text-muted">
        Maximum number of tokens to generate in responses.
        Default: 2048
      </small>
    </div>
  </fieldset>

  <div class="form-actions">
    <%= f.submit class: "btn btn-primary" %>
    <%= link_to "Cancel", admin_llm_providers_path, class: "btn btn-link" %>
  </div>
<% end %>
